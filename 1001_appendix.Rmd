
# Appendix

## Data set descriptions

### McClelland 

#### Description

- McClelland et al. (2013) abstract 

> This study examined relations between children's attention span-persistence in preschool and later school achievement and college completion. Children were drawn from the Colorado Adoption Project using adopted and non-adopted children (N = 430). Results of structural equation modeling indicated that children's age 4 attention span-persistence significantly predicted math and reading achievement at age 21 after controlling for achievement levels at age 7, adopted status, child vocabulary skills, gender, and maternal education level. Relations between attention span-persistence and later achievement were not fully mediated by age 7 achievement levels. Logistic regressions also revealed that age 4 attention span-persistence skills significantly predicted the odds of completing college by age 25. The majority of this relationship was direct and was not significantly mediated by math or reading skills at age 7 or age 21. Specifically, children who were rated one standard deviation higher on attention span-persistence at age 4 had 48.7% greater odds of completing college by age 25. Discussion focuses on the importance of children's early attention span-persistence for later school achievement and educational attainment.

#### Reference

McClelland, Acock, Piccinin, Rheac, Stallings. (2013). Relations between preschool attention span-persistence and age 25 educational outcomes. [link](http://www.sciencedirect.com/science/article/pii/S0885200612000762) Note that there is only one age 25 outcome (college completion) and two age 21 outcomes.

The following models duplicate the paper results. Additional non-mediation models are provided for comparison.

```{r, eval=FALSE}
modReading = "
  read21 ~ rr*read7 + ar21*attention4 + vocab4 + male + adopted + momed
  read7 ~ ar7*attention4

  # in
  att4_read21 := ar7*rr
"
reading  = sem(modReading, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
summary(reading, rsquare=TRUE)

modRead = "
  read21 ~ read7 + attention4 + vocab4 + male + adopted + momed

"
readnomed  = sem(modread, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
AIC(read, readnomed)


modMath = "
  math21 ~ mm*math7 + am21*attention4 + vocab4 + male + adopted + momed
  math7 ~ am7*attention4

  # in
  att4_math21 := am7*mm
"
math  = sem(modMath, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
summary(math, rsquare=TRUE, fit=T)

modMath = "
  math21 ~ math7 + attention4 + vocab4 + male + adopted + momed

"
mathnomed  = sem(modMath, data=mcclelland, missing='fiml', mimic = 'Mplus', std.ov=TRUE, se ='boot')
AIC(math, mathnomed)
```


### National Longitudinal Survey of Youth (1997, NLSY97)

#### Description

NLSY97 consists of a nationally representative sample of approximately 9,000 youths who were 12 to 16 years old as of December 31, 1996. Round 1 of the survey took place in 1997. In that round, both the eligible youth and one of that youth's parents received hour-long personal interviews. In addition, during the screening process, an extensive two-part questionnaire was administered that listed and gathered demographic information on members of the youth's household and on his or her immediate family members living elsewhere. Youths are interviewed on an annual basis. 

The NLSY97 is designed to document the transition from school to work and into adulthood. It collects extensive information about youths' labor market behavior and educational experiences over time. Employment information focuses on two types of jobs, "employee" jobs where youths work for a particular employer, and "freelance" jobs such as lawn mowing and babysitting. These distinctions will enable researchers to study effects of very early employment among youths. Employment data include start and stop dates of jobs, occupation, industry, hours, earnings, job search, and benefits. Measures of work experience, tenure with an employer, and employer transitions can also be obtained. Educational data include youths' schooling history, performance on standardized tests, course of study, the timing and types of degrees, and a detailed account of progression through post-secondary schooling.

#### Reference

[Bureau of Labor Statistics](http://www.bls.gov/nls/nlsy97.htm)


### Wheaton 1977 data 

#### Description

> Longitudinal data to develop a model of the stability of alienation from 1967 to 1971, accounting for socioeconomic status as a covariate. Each of the three factors have two indicator variables, SES in 1966 is measured by education and occupational status in 1966 and alienation in both years is measured by powerlessness and anomia.

#### Reference

Wheaton, B., Muthen B., Alwin, D., & Summers, G., 1977, "Assessing reliability and stability in panel models", in D. R. Heise (Ed.), _Sociological Methodology 1977_ (pp. 84-136), San Francisco: Jossey-Bass, Inc. 


### Old Faithful

#### From the R helpfile

Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. A closer look at faithful$eruptions reveals that these are heavily rounded times originally in seconds, where multiples of 5 are more frequent than expected under non-human measurement. For a better version of the eruption times, see the example below.

There are many versions of this dataset around: Azzalini and Bowman (1990) use a more complete version.

### Harman 1974

#### Description

A correlation matrix of 24 psychological tests given to 145 seventh and eight-grade children in a Chicago suburb by Holzinger and Swineford.

#### Reference

Harman, H. H. (1976) Modern Factor Analysis, Third Edition Revised, University of Chicago Press, Table 7.4.




## Terminology in SEM

SEM as it is known has been widely used in psychology and education for decades, while other disciplines have developed and advanced techniques that are related, but would not typically call them SEM.  The following will be expanded over time.

**Latent variable**: an unobserved or hidden variable.  It's specific interpretation will depend on the modeling context.

**Factor Analysis**: in the SEM literature, this refers to a latent variable (measurement) model to assess the underlying construct behind the correlations among a set of observed variables.  Elsewhere it may refer to a very broad family of matrix factorization techniques that would include things like principal components analysis, non-negative matrix factorization, etc.

**Exploratory vs. Confirmatory**: This distinction is problematic.  Science and data analysis is inherently exploratory, and most who use SEM do some model exploration as they would with any other model.  Some SEM models have more constraints than others, but that does not require a separate name or way of thinking about the model.

**Mediation** and **moderation**:  These mean different things, both straightforward, and which is utilized in a model should be based on theoretical notions.

- Mediation: an indirect effect, e.g. A->B->C, A has an indirect effect on C. A can have a direct effect on C too. 
- Moderation: an interaction (the same ones utilized in a standard regression modeling)

See the terminology section in the graphical models chapter for more detail.

**Fit**: Model fit is something very difficult to ascertain in SEM, and notoriously problematic in this setting, where all proposed cutoffs for a good fit are ultimately arbitrary.  Even if one had most fit indices suggesting a 'good' fit, there's little indication the model has predictive capability.

**Endo/Exogenous**: Endogenous variables are determined by other variables, i.e. have an arrow pointing at their node. Exogenous variables have no analyzed causes.

**Disturbance**: residual variance. Includes measurement error and unknown causes.

**Mixture Models**: models using categorical latent variables.

## Parallel Process Example

```{r parallelProcess, eval=FALSE}

# parallel process --------------------------------------------------------
# See EXAMPLE 6.13 in Mplus User's Guide

# let's simulate data with a negative slope average and positive correlation among intercepts and other process slopes
set.seed(1234)
n = 500
timepoints = 4
time = rep(0:3, times=n)
subject = rep(1:n, each=4)

# first we'll draw intercepts with overall mean .5 and -.5 for the two
# processes, and let them have a slight correlation. Their variance is 1.
intCorr = matrix(c(1,.2,.2,1), ncol=2) 
colnames(intCorr) = rownames(intCorr) = c('i1', 'i2')
intCorr

interceptP1 = .5
interceptP2 = -.5

ranInts = MASS::mvrnorm(n, mu=c(0,0), Sigma = intCorr, empirical=T)
ranInts = data.frame(ranInts)
head(ranInts)
cor(ranInts)
colMeans(ranInts)

# now create slopes with intercept/mean .4, -.4, but the same positive
# relationship with their respective intercept. Their variances are also 1.
slopeP1 = .4
slopeP2 = -.4

s1 = .3*ranInts$i2  + rnorm(n)
s2 = .3*ranInts$i1  + rnorm(n)

ranef = data.frame(ranInts, s1, s2)
head(ranef)


# so we have slight positive correlations among all random intercepts and slopes
y1 = (interceptP1 + ranef$i1[subject]) + (slopeP1+ranef$s1[subject])*time + rnorm(n*timepoints, sd=.5)
d1 = data.frame(Subject=subject, time=time, y1)
head(d1)

library(ggplot2)
ggplot(aes(x=time, y=y1), data=d1) + 
  geom_line(aes(group=Subject), alpha=.1) + 
  geom_smooth(method='lm',color='red') +
  lazerhawk::theme_trueMinimal()


y2 = (interceptP2 + ranef$i2[subject]) + (slopeP2+ranef$s2[subject])*time + rnorm(n*timepoints, sd=.5)
d2 = data.frame(Subject=subject, time=time, y2)

# process 2 shows the downward overall trend as expected
ggplot(aes(x=time, y=y2), data=d2) + 
  geom_line(aes(group=Subject), alpha=.1) + 
  geom_smooth(method='lm',color='red') +
  lazerhawk::theme_trueMinimal()

# Widen from long form for lavaan
library(tidyr)
negslopepospath1 = d1 %>% spread(time, y1)
colnames(negslopepospath1) = c('Subject', paste0('y1', 1:4))
head(negslopepospath1)

negslopepospath2 = d2 %>% spread(time, y2)
colnames(negslopepospath2) = c('Subject', paste0('y2', 1:4))

# combine
dparallel = dplyr::left_join(negslopepospath1, negslopepospath2)
head(dparallel)

mainModel = "
i1 =~ 1*y11 + 1*y12 + 1*y13 + 1*y14
s1 =~ 0*y11 + 1*y12 + 2*y13 + 3*y14


i2 =~ 1*y21 + 1*y22 + 1*y23 + 1*y24
s2 =~ 0*y21 + 1*y22 + 2*y23 + 3*y24

s1 ~ i2
s2 ~ i1
"

library(lavaan)
mainRes  = growth(mainModel, data=dparallel)
summary(mainRes)
fscores = lavPredict(mainRes)
broom::tidy(data.frame(fscores))
lm(s2~., fscores)

lazerhawk::corrheat(cor(fscores))
qplot(s1, i2, data=data.frame(fscores)) + geom_smooth(method='lm', se=F)
fv = lavPredict(mainRes, 'ov')
summary(mainRes, standardized=T)
d3heatmap::d3heatmap(cor(fv, fscores))
d3heatmap::d3heatmap(cor(select(dparallel, -Subject), ranef), Rowv = F, Colv = F)


process1Model = "
i1 =~ 1*y11 + 1*y12 + 1*y13 + 1*y14
s1 =~ 0*y11 + 1*y12 + 2*y13 + 3*y14
"
p1Res = growth(process1Model, data=dparallel)
fscoresP1 = lavPredict(p1Res)

process2Model = "
i2 =~ 1*y21 + 1*y22 + 1*y23 + 1*y24
s2 =~ 0*y21 + 1*y22 + 2*y23 + 3*y24
"
p2Res = growth(process2Model, data=dparallel)
fscoresP2 = lavPredict(p2Res)

fscoresSeparate = data.frame(fscoresP1, fscoresP2)

pathMod = "
s1 ~ i2
s2 ~ i1

i1~~i2
"

pathModRes = sem(pathMod, data=fscoresSeparate, fixed.x = F)
summary(pathModRes)  # you'd have come to the same conclusions
summary(mainRes)
```



## Causal Bias

> Consider what effects, that might conceivably have practical bearings, we conceive the object of our conception to have. Then, our conception of these effects is the whole of our conception of the object. 

I figure I should note my stance on soi-disant *causal modeling* so that whatever I might say in this document is taken with the appropriate context. What follows is more or less a philosophical stance, perhaps a naïve and not very well developed one at that, despite my philosophy background from long ago, but one that I think is a safer perspective than others commonly held regarding causes and statistics.  Mostly this is just me jotting down some things I'm thinking about while working on this document, and perhaps I'll be able to spend more time with it later.

To begin, no statistical model *by itself* will ever provide evidence of causal effects. This is a fact that cannot be disputed.  Statistical models of the sort we are usually interested in are inherently probabilistic, atheoretical in and of themselves, and wholly dependent on the data collected. No amount of fancy analysis or double-blind randomization will change the fact that in the end you have a probabilistic result, and one that is highly dependent on many assumptions both statistical and theoretical, as well as nuances of the data.  The data itself might suggest several statistical models that are essentially if not exactly equally adequate.  If you are using SEM or other approach to *discover* causal effects you will thus be unsuccessful, and as such, you should not be using the technique if that is the primary reason for doing so.

Philosophically speaking, I also don't think the methods of scientific analysis, i.e. statistics, can be used to *prove* causal relations, and more so if one considers that we've been debating the nature of causality for thousands of years and have yet to come to a conclusion about its definition that everyone can agree on.  Such bold 'proof-like' and 'causal' claims, consistently shown to be wrong because they *must be* in order to be a scientific claim in the first place, have much to do with undermining public faith in scientific results.  However, if we can't entirely agree on what constitutes a causal relation in the first place, we definitely can't hope that there is some magical statistical technique that would help us determine it.

As an example, available data should not convince you that smoking causes lung cancer, and that is because it doesn't.  If it did, everyone who ever smoked would have lung cancer.  This is a *deterministic* notion of causality, and there are others, but it is the one I think is used in everyday parlance. Using a *probabilistic* interpretation instead, e.g. a smoker is more likely to have cancer, just serves to emphasize the uncertainty that exists in the underlying model.  I'm perfectly okay with this, but many seem uncomfortable with any lack of certainty.  I might even say that smoking has an 'effect' on the likelihood of getting lung cancer[^afrsmoke]. In the end, all we have in the data are those that have lung cancer or not, and there is no uncertainty about them having it, nor does our knowledge of their smoking habits change the fact of whether they do.

As another example, you can randomly assign people who have cancer to two groups- in one they take an aspirin every day, in the other they drink orange juice every day. You may then find that they are equally effective in terms of remission rates, but it would be silly to think the 'treatments' had any causal effect at all, even though the effects could be non-zero statistically. Randomization, which is assumed by many to have the magical ability to confer causality on a scientific endeavor, doesn't help the situation either.  In fact, it could be said that *control* is the key element in determining causal relations (a claim that like others is not without issue), and the random nature of assignment actually undermines that.  

Modern health science is not removed from this issue, or even far removed from this example, and regularly finds 'effects' of drugs or behavior that have no causal relation to the phenomenon under study. This is especially the case with psychological phenomena, many of which to this day still have little or no tie to operational definitions based on physiology rather than behavior.  People even go so far as to ascribe an actual causal effect to nothing at all (placebo effect).

I personally think it's somewhat misguided to think that a major goal of (at least most of working) science is to establish formal causal relations.  Its primary tool for weighing evidence, i.e. statistical modeling, certainly cannot do that. As much as we want control, which is one thing that could potentially establish causality, it eludes us, and problematically seems to beg for a human element to causality besides.  *Ceteris peribus* can only work for what is included in our models.  Furthermore, if we actually knew the cause of something, we definitely would not need a statistical model.  On the practical side, few seem to be engaging in science for reasons of determining causal effects (except perhaps in the discussion sections of papers), and rather do so to discover new potential explanations and predict something better than we did before.


### Prediction

A well-worn data set on which to test classification algorithms is the [MNIST hand written digits data](https://en.wikipedia.org/wiki/MNIST_database).  The methods use the pixel gray-scale information to classify an image of what are originally handwritten digits as being one of the values 0-9.  Modern approaches can get accurate classification *on test data* with less than 1% error.  If one's goal is to understand why the digits are the way they are, this model cannot help you. And yet, a statistical approach can be very successful while still having nothing to say about the causal mechanisms influencing the target variable we wish to speak about. 

If you can predict something with 99% accuracy, how much are you concerned about the underlying causal reality? But this is in fact what I think is the primary *practical* goal of scientific endeavor, i.e. accurate prediction. It definitely is not about 'true' parameters and p-values.  The models are wrong, but they can work, and we'd prefer those that work better. That is something science can provide.  Models can even correspond to 'reality' as we currently know it, but we all know that knowledge of reality changes as well, often due to science scientific discoveries. Peirce figured this out long ago. I'd personally rather have something that works than have one of the 'true causes' and a model that leaves me no better than guessing.  Now let's say you have a 'causal' model and another model that uses other covariates, and yet they both predict equally well. What would be the basis for preferring one over the other? I think we'd all prefer the causal model, but what would tangibly be gained in the preference?


### Chance

Another related point, *chance* definitely does not cause anything. There are mechanisms you do not know about contributing to the variability in the subject matter under study, but nothing from your results are *due to chance*.  Statistical models will always have uncertainty regarding them, and how much or how little there is depends on many things.  But just because we don't know what's going on, we cannot put the unknown as due to some magical notion akin to coincidence.


### Other

- There are people, who have been taken seriously in other contexts, actively devoting time, money and energy to determine a. that our current existence is a simulation, and b. to find a way to 'break out of it' (I suspect this has to do with them taking their experience at Burning Man too seriously).  However useless an endeavor this may be, if it was a simulation, where would any theory about causality reside then?

- An interesting point noted by Pearl is that causal assumptions are not encoded via the paths/edges, which only note possible causal relations, but in the missing ones.  This reminds me of a chapter of the *Tao Te Ching*, that talks about the effectiveness of nothingness.  An example being that it is not the walls or physical aspects of a house, but the rooms, i.e. spaces, that make it an effective home.

- If one adheres strictly to the *strong* causal assumptions for SEM, it is difficult for me to see where any discovery or exploration comes in.  In that perspective, one uses statistical tools to merely uncover the parameter values that are assumed to be non-zero.  I don't even see why one would even reference statistical significance, interval estimates etc., as the causal aspects are assumed to be true.  I'm not sure how the sampling variability, which could potentially result in conflicting causal understanding, is incorporated. <br>
If you go to the *weak* causal assumption, where the parameter can take on a potentially infinite range of values, we now have something that is more practical and amenable to statistical analysis as traditionally engaged.  However, I don't know what definition of causality it is consistent with.  An example would be that for some model, one of the causal assumptions is that X causes Y, and that relationship may be weak or strong, positive or negative, possibly even indistinguishable from zero, but we won't know until we look at the data.  In other words, the effect could be A, its opposite, or not A.  This is in fact how SEM is practiced the vast majority of the time.  However, that doesn't sound like any causal claim is being made at all to me, and instead merely posits some correlation to be explored.  I'm fine with that, but I'd take issue that some causal assumption is given more credibility by the statistical result, which could have been anything and still have been consistent with the 'prior knowledge'.

- As noted previously, the strong causal claims in SEM come from where we set paths equal to zero.  However, a far more realistic assumption is that every effect is non-zero, however trivially small.  Such models would be unidentifiable in SEM though.



### Some references

Practically anything by **Judea Pearl**, even though some of the above might seem like I'm entirely missing many of the points he tries to make.  On the contrary, I actually find it hard to disagree with Pearl, I just prefer to focus on the practical aspects of modeling, which in the end includes a statistical model that can exist independently of any causal notion, and on ways of improving such models.  He is very clear however, that structural models rest on untestable assumptions and statistical results.

The quote at the beginning is by **C.S. Peirce**.  I note it because I am more interested in clear ideas, without which one cannot hope to even begin to understand causal relations, however defined.  The notion of a thing intimately entails *all* of its *practical effects*.  In other words, my definition of you must include how you are potentially able to act upon the universe, and in fact, my definition of you *is* what practical effects you have on the universe.  The lack of clear ideas in scientific research is a fundamental problem too often ignored.

Philosophical, such as Aristotle, Hume, non-Western as well.



## Resources

This list serves only as a starting point, geared toward those that would be taking the course, though may be added to over time.


### Graphical Models

[Judea Pearl's website](http://bayes.cs.ucla.edu/jp_home.html): Includes papers and technical reports.

[UseR Series](http://link.springer.com/bookseries/6991): Contains texts on graphical models, Bayesian networks, and network analysis.


### Potential Outcomes

[Imai's website](http://imai.princeton.edu/projects/mechanisms.html): Papers and other info.


### Measurement Models

[Personality Project](http://personality-project.org/index.html): William Revelle's website and text on psychometric theory.


### SEM

Kline, Rex. *Principles and Practice of Structural Equation Modeling*. A very applied introduction that covers a lot of ground.  The latest edition finally includes explicit discussion of the more general graphical modeling framework within which SEM resides.

Beaujean, A. A. (2014). [Latent variable modeling using R: A step by step guide](http://blogs.baylor.edu/rlatentvariable/). New York, NY: Routledge/Taylor and Francis. Lavaan based guide to SEM


### lavaan

[lavaan website](http://lavaan.ugent.be/)

[Tutorial](http://lavaan.ugent.be/tutorial/tutorial.pdf) 

[Bayesian estimation with lavaan](https://cran.r-project.org/package=blavaan)

[Complex surveys with lavaan](https://cran.r-project.org/package=lavaan.survey)

[Interactive lavaan](https://github.com/kylehamilton/lavaan.shiny)


### Other SEM

[semTools]: Excellent set of tools for reliability assessment, measurement invariance, fit, simulation etc.

[semPlot]: Visualize your lavaan models.


[^afrsmoke]: Though it might be less of an effect than simply [being African-American](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0050185), something I'm sure we can agree is not a causal effect.