# Bayesian Nonparametric Models

IN PROGRESS

Todo: wrap up ibp, do summary and more on packages; final cleanup

In mixture models and factor analysis we must set the number of clusters or factors respectively in order to run the models.  The natural question is obviously *how many clusters* or *how many factors*? There is no answer[^42].  For standard cluster analyses like k-means and standard factor analysis, crude measures may be provided but they are not very satisfactory. Model based approaches like mixture models and SEM may allow for information-based model comparison or similar, but are still problematic.

Bayesian nonparametric approaches allow for a different approach. Instead of predetermining the number of clusters or latent variables, their numbers are allowed to grow with the data itself, or rather, the number is assumed infinite and the addition of latent classes/variables grows with the data.  The section provides a very brief overview of the techniques, and essentially summarizes the Gershamn & Blei tutorial (2011).


## Chinese Restaurant Process

The Chinese Restaurant Process (CRP) regards the extension of (finite) [mixture models][Mixture Models] previously discussed, but where we now consider infinite capacity models.  The idea is to imagine a restaurant[^chinese] with an infinite number of tables, where the tables represent our clusters. Now come the customers.  The first sits at the first table.  The next sits at that table with probability $1/(1+\alpha)$, or the next table with $\alpha/(1+\alpha)$. The process continues for the n^th^ person, who sits at any occupied table with probability $m_k/(n-1+\alpha)$ where $m_k$ is the number of people sitting at table $k$, and at a new table $\alpha/(n - 1+\alpha)$.  In other words, a person sits at the occupied tables with some probability proportional to the number of people already sitting there, and the unoccupied table at some probability proportional to $\alpha$.  The choice of the parameter determines how likely new customers are to join an occupied or new table.

It is perhaps instructive, at least for those more familiar with R or other programming, to see the code for this process.  The following code is in no way optimized, and serves only for demonstration. One can set the alpha parameter and number of observations.

```{r crpRaw}
crp <- function(alpha, n) {
  table_assignments = 1
  
  for (i in 2:n){
    table_counts = table(table_assignments)         # counts of table assignments
    table_prob = table_counts/(i-1+alpha)           # probabilities of previous table assignments
    # sample assignment based on probability of current tables and potential next table
    current_table_assignment = sample(seq_len(length(table_counts)+1), 1, 
                                      prob=c(table_prob, 1-sum(table_prob)))
    # concatenate to previous table assignments
    table_assignments = c(table_assignments, current_table_assignment)  
  }
  table_assignments
}
```

```{r debugcrp, echo=FALSE, eval=FALSE}
# if desired
debugonce(crp)
crp(alpha=1, n=10)
```


Setting a higher $\alpha$, sometimes called the concentration parameter, allows new table assignments to be made more easily.

```{r crpPlot, dev='svglite', echo=c(1:2, 7)}
n = 100
crp_1 = crp(alpha=1, n=n)
test = matrix(0, nrow=n, ncol=n_distinct(crp_1))
for (i in 1:n_distinct(crp_1)){
  test[, i] = ifelse(crp_1==i, 1, 0)
}
crp_1 = test

crp_4 = crp(alpha=4, n=n)
test = matrix(0, nrow=n, ncol=n_distinct(crp_4))
for (i in 1:n_distinct(crp_4)){
  test[, i] = ifelse(crp_4==i, 1, 0)
}
crp_4 = test
# 
# crp(alpha=1, n=100) %>% table %>% barplot(bty='n', col='#ff5503', border=NA, pch=19, col.axis='gray25', main=expression('alpha = 1'), col.main='gray25', xlab='Cluster', ylab='Frequency', col.lab='gray10')
# crp(alpha=4, n=100) %>% table %>% barplot(bty='n', col='#ff5503', border=NA, pch=19, col.axis='gray25', main=expression('alpha = 5'), col.main='gray25', xlab='Cluster', ylab='Frequency', col.lab='gray10')


d3heatmap::d3heatmap(crp_1, Rowv=F, Colv=F, yaxis_font_size='0pt', colors='Blues')
d3heatmap::d3heatmap(crp_4, Rowv=F, Colv=F, yaxis_font_size='0pt', colors='Blues')
```

Going back to the finite mixture model setting, the CRP defines the prior for category membership in that context, but without assuming a number of classes beforehand.  As in the standard mixture model, any number of parameters $\theta_k$ may serve to define each of the $k$ categories. In addition, in the Bayesian context, a hyperprior may be placed on an unknown $\alpha$.  


## Indian Buffet Process

The so-called Indian Buffet Process (IBP) is the continuous latent variable counterpart to the CRP. The idea is that of sampling an infinite number dishes at an Indian restaurant buffet, where each dish represents a latent factor.  Like sitting at tables in the CRP, customers are more likely to sample dishes that have already been sampled. One key difference is that the customers are the observed variables or measures, and may be associated with any number of dishes (latent factors), whereas in the CRP, data points are assigned to only one latent class.


```{r ibp0, eval=F, echo=F}
ibp = function(alpha, N){
  # original at https://github.com/mcdickenson/shinyapps/tree/master/ibp-demo
  # preallocate assignments with upper bound pf N*alpha on expected number of columns 
  assignments = matrix(NA, nrow=N, ncol=N*alpha) 
  
  # start with some dishes
  # dishes = rpois(1, alpha)      # this will result in an error for dishes_from_previously_sampled if zero
  dishes = max(1, rpois(1, alpha))      # fixed so it won't start with zero (we need at least one dish or there is no buffet!)
  zeroes = ncol(assignments) - dishes   # fill in the rest of potential dishes
  
  assignments[1,] = c(rep(1, dishes), rep(0, zeroes))
  
  for(i in 2:N){
    prev = i-1
    # last_previously_sampled_dish = max(which(assignments==1, arr.ind=T)[, 'col'])
    last_previously_sampled_dish = sum(colSums(assignments[1:prev,,drop=F])>0)    # does the same but would be faster
    dishes_from_previously_sampled = matrix(NA, nrow=1, ncol=last_previously_sampled_dish)
    
    for(k in 1:last_previously_sampled_dish){
      m_k = sum(assignments[1:prev, k])                     # before it was 1:i-1, which would attempt to index 0, though had no effect
      prob = m_k / i
      dishes_from_previously_sampled[1, k] = rbinom(1, 1, prob)
    }
    
    new_dishes = rpois(1, alpha/i)
    zeroes = ncol(assignments) - (last_previously_sampled_dish + new_dishes)
    assignments[i,] = c(dishes_from_previously_sampled, rep(1,new_dishes), rep(0, zeroes))
  }
  
  last_sampled_dish = sum(colSums(assignments[1:prev,,drop=F])>0) 
  assignments = assignments[1:N, 1:last_sampled_dish]
  
  return(assignments)
}

debugonce(ibp)
test = ibp(1, 100)
test = replicate(100, ibp(10, 100))

```


```{r ibp}
ibp = function(alpha, N){
  # original at https://github.com/mcdickenson/shinyapps/tree/master/ibp-demo
  # changes were made to fix potential errors, speed up and just subjectively
  
  # preallocate assignments with upper bound pf N*alpha on expected number of columns 
  assignments = matrix(NA, nrow=N, ncol=N*alpha) 
  
  # start with some dishes
  dishes = rpois(1, alpha)      
  zeroes = ncol(assignments) - dishes   # fill in the rest of potential dishes
  
  assignments[1,] = c(rep(1, dishes), rep(0, zeroes))
  
  for(i in 2:N){
    prev = i-1
    last_previously_sampled_dish = sum(colSums(assignments[1:prev,,drop=F]) > 0)    # shortcut/speedier approach
    dishes_from_previously_sampled = matrix(NA, nrow=1, ncol=last_previously_sampled_dish)

    prob = colSums(assignments[1:prev, 1:last_previously_sampled_dish, drop=F]) / i
    dishes_from_previously_sampled[1,]  = sapply(prob, rbinom, n=1, size=1)

    new_dishes = rpois(1, alpha/i)
    zeroes = ncol(assignments) - (last_previously_sampled_dish + new_dishes)
    assignments[i,] = c(dishes_from_previously_sampled, rep(1,new_dishes), rep(0, zeroes))
  }
  
  last_sampled_dish = sum(colSums(assignments[1:prev,,drop=F]) > 0) 
  assignments = assignments[1:N, 1:last_sampled_dish]
  
  return(assignments)
}
```

```{r testibp, echo=FALSE, eval=FALSE}
debugonce(ibp)
test = ibp(1, 100)
test = replicate(100, ibp(1, 100))
```


With the above code we can demonstrate the IBP with a setting that would make the creation of more latent variables more difficult, and one less so. The product of the process is binary matrix where rows represent the measures and columns the latent factors.

```{r plotIBP, echo=2:3}
set.seed(123)
ibp_1 = ibp(1, 100)
ibp_4 = ibp(4, 100)
d3heatmap::d3heatmap(ibp_1, Rowv=F, Colv=F, yaxis_font_size='0pt', colors='Blues')
d3heatmap::d3heatmap(ibp_4, Rowv=F, Colv=F, yaxis_font_size='0pt', colors='Blues')
```

Both have the same number of measures. However we see that with the initial result, most stick to a few factors, while in the latter they are more likely to sample additional dishes.

So now that we have the binary, on-off, switch, $Z$, that results from the IBP, we can think of the usual factor loading matrix $\Lambda$ as decomposed into the $Z$ and weight matrix $w$,  and then we are [as we were before][Constructs and Measurement Models] with observed variables X as a weighted combination of the latent factors[^maskondata].


$$Z \odot w = \Lambda$$ 
$$X_n =  \Lambda F_n + \epsilon_n$$

The above assumes an observed data matrix $X$, where each $n$ observations is of some dimension $m$.  $\Lambda$ is our $m$ by $k$ loading matrix, where $k$ is the number of latent factors, and is created by the elementwise product of $Z$ and $w$. $F$ represents our factors, and $\epsilon$ the noise.

## Summary



## R packages used

None.  There are numerous packages for the CRP if one looks for 'bayesian nonparametric' or 'dirichlet process', though there are quite a few variants of the underlying process possible too. In addition, many of them would be allow usages as with the flexmix package, where model parameters can vary over the latent clusters.




[^42]: There is no answer because they aren't real.

[^chinese]: The name comes from an early paper whose authors were referencing restaurants in San Francisco.

[^maskondata]: This follows the presentation in Gershamn & Blei tutorial (2011) and Knowles & Ghahramani (2011).  In other instances it is depicted similar to the CRP, where the binary mask matrix Z regards the latent factors (e.g. Knowles & Ghahramani (2007)), i.e. it focuses on the data observations rather than the data dimensions.