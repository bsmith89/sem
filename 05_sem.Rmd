# Structural Equation Modeling

Structural equation modeling combines the path analytic and latent variable techniques together to allow for regression models among latent and observed variables.  Any model, even the SLiM, can be seen as some form of SEM, or graphical model more generally.  However, the term is typically reserved for the combination of latent and observed variables in a model, often with causal implications[^bpSEM].


## Measurement Model

The <span class="emph">measurement model</span> refers to the latent variable models, i.e. factor analysis, and typical practice in SEM is to investigate these separately and first.  The reason is that one wants to make sure that the measurement model holds before going any further with the underlying constructs. For example, for one's sample of data one might detect two latent variables work better for a set of indicators, or might find that some indicators are performing poorly.  Adjustments might then be made before moving on to the final model.


## Structural Model

The <span class="emph">structural model</span> specifies the relations among latent and observed variables that do not serve as indicators.  It can become quite complex, but at this stage one can lean on what they were exposed to with path analysis, as conceptually we're in the same place, except now some variables may be latent.


## The Process

### Initial Considerations of Complexity

Before even beginning a road to SEM, do you have enough data? 

> <span class="" style="font-variant:small-caps">SEM is a large sample technique</span>

You will even in simple models likely be estimating a couple dozen parameters.  If you were running a standard regression, how much data would you feel comfortable with if you were using a model with 20+ predictors? In SEM it can be even more difficult, where latent variables with less than ideal loadings on few items might require a couple hundred observations for just a single measurement model. In addition, as noted previously, less reliable measures will only hurt your ability to detect the relationships among variables.  If you are conducting multigroup analysis, you must consider an SEM from the standpoint of the sample size for the smallest group.

Because of the inherent complexity of SEM, many run simpler models that are implausible, and actually misspecified, due to the fact that key variables, even ones that were collected and available in the data, are left out[^notv].  Sometimes models are so complex they are left out because it would be difficult to even know what all should be pointing to what.  In other cases I've seen almost a hundred parameters estimated with maybe only 200 observations[^notbayes]. The gist is that SEM is not always the right tool, even if conceptually it may be considered for a model.  As flexible as it is, it is also quite restrictive in other respects, and may simply be impractical to model key aspects of the data.


### Steps to Take

Assuming SEM is the right tool for the job, the first analytical step is to look at *all* the observed variables on their own by examining their descriptive statistics.  This also helps with debugging, such that you can find data entry errors and other issues before they get lost amidst the sea of output from an SEM result. And you certainly should know your data well enough to know that a value of 50 isn't possible for a scale that goes 0-40.

Next comes an examination of the correlations, possibly broken up to be more digestible or reflect item scales.  SEM does not supernaturally produce correlations in your data, it works on the correlations that are there.  If there are very low correlations, you will have a poor fit when you do the SEM, and possibly even estimation problems. This should not in any way be surprising.

Now you are ready to consider your measurement models. This is a two stage process (at least). First, consider every latent variable on its own, i.e. examine whether the single factor models hold up.  Then you might allow those that are supposed to correlate in some fashion (whether it is a regression relationship or not doesn't matter), maybe even with observed variables.  Again, if you are spotting potential issues here, they aren't going to go away later.  Note that if you're using <span class="pack">lavaan</span> (or Mplus), you can simply write the syntax for a model with constraints specifying zero correlations, then just comment those lines out (recall that latent variables are allowed to correlate by default). 

Now you are ready for the SEM, though by this stage you should already be pretty knowledgeable about what to expect.  You should always have multiple models, and I can think of at least three in any SEM setting.  The first should be the simplest-yet-still-theoretically-plausible model.  This would likely resemble a standard regression model, though including latent variables, and possibly multiple outcomes (but at this stage left uncorrelated and considered separately).  The other model is the complex one that might include indirect effects and other posited correlations, i.e. the one that drove you to do the analysis in the first place. Ideally there would also be a model based on a competing theory, and if so, you should definitely run that. And finally, a data driven model. Perhaps your initial explorations of the data, or your interpretation of one of the other two models, suggested something else going on.  Try it out, just be clear about it being more exploratory in your write up.  Theories are great and all, but nature typically proves more interesting, and you don't want to ignore intriguing results just because they didn't align with theory.


In summary:

- Have a sense of complexity (vs. sample size) and other possible techniques
- Develop a good understanding of *all* observed variables
- Inspect correlations
- Examine measurement models
- Conduct SEM with multiple competing models

SEM works best with strong causal theory and settings where those claims can be tested.  Such a setting is not nearly as common as SEM is used in practice.  While SEM can be seen as an extension of other common modeling approaches and used primarily for that, one should think hard before using it, and know that there is no data situation that requires SEM.



## Example

The following model is a classic example from Wheaton et al. (1977), which used longitudinal data to develop a model of the stability of alienation from 1967 to 1971, accounting for socioeconomic status as a covariate. Each of the three factors have two indicator variables, SES in 1966 is measured by education and occupational status in 1966 and alienation in both years is measured by powerlessness and anomia. The structural component of the model hypothesizes that SES in 1966 influences both alienation in 1967 and 1971 and alienation in 1967 influences the same measure in 1971.  We also let the disturbances correlate from one time point to the next.

```{r wheatonSetup, echo=10, results='hide'}
library(lavaan)
# The classic Wheaton et. al. (1977) model 
# panel data on he stability of alienation
lower <- '
 11.834,
  6.947,    9.364,
  6.819,    5.091,   12.532,
  4.783,    5.028,    7.495,    9.986,
 -3.839,   -3.889,   -3.841,   -3.625,   9.610,
-21.899,  -18.831,  -21.748,  -18.775,  35.522,  450.288 '
 
# convert to a full symmetric covariance matrix with names
wheaton.cov <- getCov(lower, names=c("anomia67","powerless67", "anomia71",
                                     "powerless71","education","sei"))
 
# the model
wheaton.model <- '
  # measurement model
    ses     =~ education + sei
    alien67 =~ anomia67 + powerless67
    alien71 =~ anomia71 + powerless71
 
  # structural model
    alien71 ~ aa*alien67 + ses
    alien67 ~ sa*ses
 
  # correlated residuals
    anomia67 ~~ anomia71
    powerless67 ~~ powerless71

  # Indirect effect
    IndirectEffect := sa*aa
'
 
alienation <- sem(wheaton.model, sample.cov=wheaton.cov, sample.nobs=932)
```


```{r wheatonPlot, eval=F, echo=FALSE, dev='svglite'}
# keep this
semPlot::semPaths(alienation, layout='tree2', style='lisrel', whatLabels = 'path', groups=c('latent', 'man'), 
                  color=list(latent='#ff5503', man=scales::alpha('#ff5503',.5)),
                  sizeMan=5, sizeLat=10, edge.label.cex=.75, borders=F, #edge.color='#1e90ff',
                  label.color='#ffffff', residScale=10, curve=1, nCharNodes=6, exoVar=T, covAtResiduals=T, residuals=T) 
# semPlot::semPaths(alienation, residuals=T, style='lisrel', edgeLabels='', residScale=10, nCharNodes=6, exoVar=T, edge.label.cex=1)
```

```{r wheatonPlot2, echo=F, dev='svglite'}
# mix colors for structural edges
# bluegreen = c(col2rgb('dodgerblue') + col2rgb('palegreen'))/2
# bluepink = c(col2rgb('dodgerblue') + col2rgb('salmon'))/2
# greenpink = c(col2rgb('palegreen') + col2rgb('salmon'))/2
# as.hexmode(round(bluegreen))
# as.hexmode(round(bluepink))
# as.hexmode(round(greenpink))

tags$div(style="width:100%; margin:auto auto;",
DiagrammeR("
digraph {

 # Intialization of graph attributes
 graph [rankdir=TB layout=dot]
 
 #### Node statements
 ## Intialization of node attributes
 node [style=filled, color=gray75,  fontcolor=gray50];
 
 ## latent variables and associated variances
 subgraph {
    rank = same;
    Alien67 [shape=circle, fontsize=10, width=1.25, height=1, fillcolor=palegreen, fontcolor=white];
    Alien71 [shape=circle, fontsize=10, width=1.25, height=1, fillcolor=salmon, fontcolor=white];
 }

 subgraph {
    rank = same;
    Alien67;
    Alien67var[shape='none' color='none' fillcolor='none' label='' minlen=1];
    Alien71var[shape='none' color='none' fillcolor='none' label='' minlen=1];
 }
 
 subgraph{
    rank=same;
    SES [shape=circle, fontsize=10, width=1.25, height=1, fillcolor=dodgerblue, fontcolor=white]; 
    SESvar[shape='none' color='none' fillcolor='none' label=''];
 }

 ## observed variables
 node [width=1, height=1, fixedsize=true, shape=square, style='']

 subgraph {
    rank=same;
    node [color=palegreen]
    anomie_67[label='Anomie\n1967'] powerless_67[label='Powerless\n1967'];
    node [color=salmon]
    anomie_71[label='Anomie\n1971'] powerless_71[label='Powerless\n1971']; 
 }

 node [color=dodgerblue]
 Edu SEI;


 ## variances
 subgraph {
    rank=same;
    node [height=0, shape=none, color=gray80]
    U1 [label='']; 
    U2 [label='']; 
    
    U3 [label='']; 
    U4 [label='']; 
 }

 subgraph {
    rank=same;
    node [height=0, shape=none, color=gray80]
    U5 [label='']; 
    U6 [label='']; 
 }

 #### Edge statements
 
 ## Initialization of edge attributes

 edge [color = gray50, rel = yields dir=forward fontsize=12]
 
 ## latent to latent

 edge [label='' fontcolor=gray25 minlen=1] # set min to 1 made it centered
 SES -> Alien67 [color='#5bc6cc'];
 SES -> Alien71 [color='#8c88b8'];

 edge [label='' fontcolor=gray25 dir=forward]
 Alien67 -> Alien71 [color='#c9be85' minlen=6];

 ## latent to observed

 edge [label='' fontcolor=gray25 ]
 {Edu SEI} -> SES [dir=back color=dodgerblue minlen=2];

 edge [label='' fontcolor=gray25 minlen=2 dir=forward]
 Alien67 -> {anomie_67 powerless_67} [color=palegreen]; 

 edge [label='' fontcolor=gray25 minlen=2]
 Alien71 -> {anomie_71 powerless_71} [color=salmon]; 

 ## variances

 SESvar -> SES[angle=45];
 Alien67var -> Alien67[angle=45];
 Alien71 -> Alien71var[angle=45 dir=back];
 
 U5 -> SEI [label='' minlen=.5];
 U6 -> Edu [label='' minlen=.5];
 
 
 edge [dir=back fontsize=8 fontcolor=gray25 minlen=.5] 
 anomie_67  -> U1 [label=''];
 powerless_67 -> U2 [label=''];
 
 anomie_71 -> U3 [label=''];
 powerless_71 -> U4 [label=''];
 
 U1 -> U3[dir=both color=gray80];
 U2 -> U4[dir=both color=gray80];
 }
 ", type='grViz', width='100%')
)
```


<br>

The standardized results of the structural model are visualized below, and model results below that. In this case, the structural paths are statistically significant, as is the indirect effect specifically.  Higher socioeconomic status is affiliated with less alienation, while there is a notable positive relationship of prior alienation with later alienation.  We are also accounting for roughly 50% of the variance in 1971 alienation.  Colors represent positive vs. negative weights, and the closer to zero the more faded they are.

```{r wheatonStructural, eval=F, echo=F, dev='svglite'}
semPlot::semPaths(alienation, residuals=T, style='lisrel', what='std', sizeLat=16,
                  residScale=10, nCharNodes=6, exoVar=T, edge.label.cex=1, 
                  structural=T, borders=F, color='#ff5503', label.color='#ffffff')
```


```{r wheatonStructural2, echo=FALSE, dev='svglite'}
tags$div(style="width:100%; margin:auto auto;",
DiagrammeR("
digraph {

 # Intialization of graph attributes
 graph [rankdir=TB layout=dot]
 
 #### Node statements
 ## Intialization of node attributes
 node [style=filled, color=gray75,  fontcolor=gray50];
 
 ## latent variables and associated variances
 subgraph {
    rank = same;
    Alien67 [shape=circle, fontsize=10, width=1.25, height=1, fillcolor=palegreen, fontcolor=white];
    Alien71 [shape=circle, fontsize=10, width=1.25, height=1, fillcolor=salmon, fontcolor=white];
 }

 subgraph {
    rank = same;
    Alien67;
    Alien67var[shape='none' color='none' fillcolor='none' label=''];
    Alien71var[shape='none' color='none' fillcolor='none' label=''];
 }
 
 subgraph{
    rank=same;
    SES [shape=circle, fontsize=10, width=1.25, height=1, fillcolor=dodgerblue, fontcolor=white]; 
    SESvar[shape='none' color='none' fillcolor='none' label=''];
 }



 #### Edge statements
 
 ## Initialization of edge attributes

 edge [color = gray80, rel = yields dir=forward fontsize=12]
 
 ## latent to latent

 edge [label='' fontcolor=gray25 minlen=1] # set min to 1 made it centered
 SES -> Alien67 [label='-.56' fontcolor='#8B00008F' color='#8B00008F', alpha=.5];   # the alpha attribute is utterly ignored for the edge so use alpha func; still will be ignored for fontcolor
 SES -> Alien71 [label='-.21' fontcolor='#8B000036' color='#8B000036'];

 edge [label='' fontcolor=gray25 dir=forward]
 Alien67 -> Alien71 [label='.57' fontcolor='#00640091' color='#00640091' minlen=6];


 ## variances

 SESvar -> SES[label='1.0'];
 Alien67var -> Alien67[label='.68'];
 Alien71 -> Alien71var[label='.50' dir=back];

 }
 ", type='grViz', width='100%')
)
```



```{r wheatonResults, echo=FALSE}
summary(alienation, fit=T, rsq=T)
```

To be clear, your interpretations based on standard regression still hold here, as the edge parameters are still regression coefficients just like they have always been. It will serve you well in the beginning to interpret each endogenous variable individually as its own model, then move toward the big picture.  Regression with latent variables is the same as regression with observed variables which is the same as a mixture of them.  Correlations, i.e. undirected paths, are still just correlations just like anywhere else.  As for the model fit indices, I will discuss them momentarily and in turn.

## Issues in SEM

### Identification

Identification generally refers to the problem of finding a unique estimate of the value for each parameter in the model. Consider the following:
$$ a + b = 2$$

There is no way for us to determine a unique solution for $a$ and $b$, e.g. the values of 1 and 1 work just as well as -1052 and 1054 and infinite other combinations.  We can talk about 3 basic scenarios, and the problem generally regards how much information we have (in terms of (co)variances) vs. how many parameters we want to estimate in the model.

- A model which has an equal number of observations (again, in terms of (co)variances) and parameters to estimate would have zero degrees of freedom, and is known as a <span class="emph">just identified</span> model. In a just identified model there are no extra degrees of freedom leftover to test model fit. 
- <span class="emph">Underidentified models</span> are models where it is not possible to find a unique estimate for each parameter. These models may have negative degrees of freedom or problematic model structures, as in the example above, and you'll generally know right away there is a problem as whatever software package will note an error, warning, or not provide output.
- <span class="emph">Overidentified models</span> have positive degrees of freedom, meaning there is more than enough pieces of information to estimate each parameter. It is desirable to have overidentified models as it allows us to use other measures of model fit.

Consider the following CFA example in which we try to estimate a latent variable model with only two observed variables, as would be the case in the prior Alienation measurement models if they are examined in isolation. We have only two variances and one covariance to estimate two paths, the latent variable variance and the two residual variances.  By convention, a path is always fixed at 1 to scale the latent variable, but that still leaves us with four parameters to estimate with only three pieces of information, hence the -1 degrees of freedom and other issues in the output.   

```{r identification, echo=-c(1:4,7,9)}
LV = rnorm(100)
x1 = 0 + .5*LV + rnorm(100, sd=sqrt(.75))
x2 = 0 + .7*LV + rnorm(100, sd=sqrt(.5))
x3 = 0 + .9*LV + rnorm(100, sd=sqrt(.2))
modelUnder = 'LV =~ x1 + x2'
modelJust = 'LV =~ x1 + x2 + x3'
library(lavaan)
underModel = cfa(modelUnder, data=cbind(x1,x2,x3))
# semPlot::semPaths(underModel, style='lisrel', exoVar=T, edge.label.cex=1, borders=F, color='#ff5503', label.color='#ffffff')
summary(underModel)
```

```{r underidentified, echo=F, dev='svglite'}
tags$div(style="width:100%; margin:auto auto;",
DiagrammeR("
digraph {

 # Intialization of graph attributes
 graph [rankdir=TB layout=dot]
 
 #### Node statements
 ## Intialization of node attributes
 node [color=gray75 fontcolor=gray50];
 
 ## latent variables and associated variances
 subgraph {
    rank = same;
    node [color=dodgerblue shape=square, fontsize=10, width=.5, height=.5, fontcolor=gray50];
    X1;
    X2;
 }

 subgraph {
    rank = same;
    node [width=.5 height=.5 shape='none' color='none' fillcolor='none' label=''];
    X1var;
    X2var;
 }
 
    LV [style='filled' shape=circle fontsize=10 width=1 height=1 fillcolor=dodgerblue fontcolor=white]; 
    LVvar[shape='none' color='none' fillcolor='none' label=''];


 #### Edge statements
 
 ## Initialization of edge attributes

 edge [color = gray80, rel = yields dir=forward fontsize=12]
 
 ## latent to observed

 edge [label='' fontcolor=gray25 minlen=1] # set min to 1 made it centered
 LV -> X1 [minlen=2 style=dashed];
 LV -> X2 [minlen=2];

 ## variances

 LVvar -> LV[];
 X2 -> X2var[dir=back];
 X1 -> X1var[dir=back];

 }
 ", type='grViz', width='100%')
)
```


Now if we had a third manifest variable, we now have $N(N+1)/2 = 3*4/2 =$ six pieces of information, i.e. 3 variances and 3 covariances, to estimate the model, and still seven unknowns. Again though, we usually fix the first loading to 1, so it would be estimated. An alternative approach would be to fix the factor variance to some value (typically 1 to create a standardized latent variable). This will allow us to estimate a unique value for each path.

Even so, this is the *just identified* situation, and so the model runs fine, but we won't have any fit measures because we can perfectly reproduce the observed correlation matrix.

```{r justID, echo=-2}
justModel = cfa(modelJust, data=cbind(x1,x2,x3))
# semPlot::semPaths(justModel, style='lisrel', exoVar=T, edge.label.cex=1, borders=F, color='#ff5503', label.color='#ffffff')
summary(justModel, fit=T)
```



```{r justid, echo=F, dev='svglite'}
tags$div(style="width:100%; margin:auto auto;",
DiagrammeR("
digraph {

 # Intialization of graph attributes
 graph [rankdir=TB layout=dot]
 
 #### Node statements
 ## Intialization of node attributes
 node [color=gray75 fontcolor=gray50];
 
 ## latent variables and associated variances
 subgraph {
    rank = same;
    node [color=dodgerblue shape=square, fontsize=10, width=.5, height=.5, fontcolor=gray50];
    X1;
    X2;
    X3;
 }

 subgraph {
    rank = same;
    node [width=.5 height=.5 shape='none' color='none' fillcolor='none' label=''];
    X1var;
    X2var;
    X3var;
 }
 
    LV [style='filled' shape=circle fontsize=10 width=1 height=1 fillcolor=dodgerblue fontcolor=white]; 
    LVvar[shape='none' color='none' fillcolor='none' label=''];


 #### Edge statements
 
 ## Initialization of edge attributes

 edge [color = gray80, rel = yields dir=forward fontsize=12]
 
 ## latent to observed

 edge [label='' fontcolor=gray25 minlen=1] # set min to 1 made it centered
 LV -> X1 [minlen=2 style=dashed];
 LV -> X2 [minlen=2];
 LV -> X3 [minlen=2];

 ## variances

 LVvar -> LV[];
 X1 -> X1var[dir=back];
 X2 -> X2var[dir=back];
 X3 -> X3var[dir=back];

 }
 ", type='grViz', width='100%')
)
```


Note that in the full alienation model, we have 6*7/2 = 21 variances and covariances, which provides enough to estimate the parameters of the model.


Determining identification is difficult for any complex model. Necessary conditions include there being model degrees of freedom $\geq 0$, and scaling all latent variables, but they are not sufficient. In general though, it is enough to know conceptually what the issue is and how the information you have relates to what you can estimate.

### Fit

There are many, many measures of model fit for SEM, and none of them will give you a definitive answer as to how your model is doing.  Your assessment, if you use them, should be based on a holistic approach to get a global sense of how your model is doing.  Let's look again at the alienation results.

```{r fitMeasures}
fitMeasures(alienation, c('chisq', 'df', 'pvalue', 'cfi', 'rmsea', 'srmr', 'AIC'))
```


#### Chi-square test

Conceptually the $\chi^2$ test measures the discrepancy between the observed correlations and those implied by the model.  In the [graphical model section](#path-analysis), we actually gave an example of reproducing a correlation from a path analysis. In general, the model goal is to reproduce them as closely as we can.  This test compares the fitted model with a (saturated) model that does not have any degrees of freedom. The degrees of freedom for this test are equal to the data (variances + covariances) minus the number of parameters estimated, and that is why when you have df=0 in a just-identified situaion, you get no fit statistics.  A non-significant $\chi^2$ suggests our model-implied correlations are not statistically different from those we observe, so yay!

```{r chisqOutput, echo=F}
cat(
'  Estimator                                         ML
  Minimum Function Test Statistic                4.735
  Degrees of freedom                                 4
  P-value (Chi-square)                           0.316
'
)
```


**Or not**.  Those familiar with null-hypothesis testing know that one cannot accept a null hypothesis, and attempting to do so is fundamentally illogical.    Other things that affect this measure specifically include multivariate non-normality, the size of the correlations (larger ones are typically related to larger predicted vs. observed discrepancies), unreliable measures (can actually make this test fail to reject), and sample size (same as with any other model scenario and statistical significance).

So if it worries you that a core measure of model fit in SEM is fundamentally problematic, good. As has been said before, no single measure will be good enough on its own, so gather as much info as you can.  Some suggest to pay more attention to the $\chi^2$ result, but to me, the flawed logic is something that can't really be overcome.  If you use it with appropriate null hypothesis testing logic, a significant $\chi^2$ test can tell you that something is potentially wrong with the model.

- Note that <span class="pack">lavaan</span> also provides a Chi-square test which compares the current model to a model in which all paths are zero, and is essentially akin to the likelihood ratio test we might use in standard model settings (e.g. comparing against an intercept only model).  For that test, we want a statistically significant result.  However, one can specify a prior model conducted with <span class="pack">lavaan</span> to test against specifically (i.e. to compare nested models).


#### CFI etc.

The <span class="emph">Comparative Fit Index</span> compares the fitted model to a null model that assumes there is no relationship among the measured items. It ranges from 0 to 1, and CFI values larger than .9 or especially .95 are typically desired, but like other completely arbitrary cutoffs those values guarantee nothing. Others include the <span class="emph">Tucker-Lewis Fit Index</span>, which is provided in standard <span class="pack">lavaan</span> output, but there are more <span class="emph">incremental fit indices</span> where those come from.

```{r cfioutput, echo=FALSE}
cat('User model versus baseline model:

  Comparative Fit Index (CFI)                    1.000
  Tucker-Lewis Index (TLI)                       0.999'
)
```


#### RMSEA

The root mean squared error of approximation is a measure that also centers on the model-implied vs. sample covariance matrix, and, all else being equal, is lower for simpler models and larger sample sizes. Maybe look for values less than .05, but again, don't be rigid with this.  <span class="pack">Lavaan</span> also provides a one-sided test that the RMSE is $\leq .05$, which ideally would be high, but the confidence interval is enough for reporting purposes.

```{r rmseaoutput, echo=FALSE}
cat('Root Mean Square Error of Approximation:

  RMSEA                                          0.014
  90 Percent Confidence Interval          0.000  0.053
  P-value RMSEA <= 0.05                          0.930

Standardized Root Mean Square Residual:

  SRMR                                           0.007')
```


#### SRMR

The standardized root mean squared residual is the mean absolute correlation residual, i.e. the difference between the observed and model-implied correlations. Historical suggestions are to also look for values less than .05, but it is better to simply inspect the residuals and note where there are large discrepancies.

```{r corResiduals}
residuals(alienation, type='cor')$cor
```

Like many matrices, it doesn't take much to where it can be difficult to discern patterns unless one takes a visual approach.  Perhaps consider <span class="pack">d3heatmap</span> (examples throughout this document) or <span class="pack">corrplot</span> packages.


#### Fit Summarized

A brief summary of these and other old/typical measures of fit are described [here](https://en.wikipedia.org/wiki/Confirmatory_factor_analysis#Evaluating_model_fit)[^fitkenny].  However they all have issues, and one should *never* use cutoffs as a basis for your ultimate thinking about model performance.  Studies have been done and all the fit indices can potentially have problems in various scenarios, and the cutoffs commonly used by applied researchers do not hold up under scrutiny.  While they can provide some assistance in the process, they are not meant to overcome a global assessment of theory-result compatibility.

### Model Comparison

All of the above, while rooted in model comparison approaches, are by themselves only providing information about the fit or lack thereof regarding *the current model only*.  In any modeling situation, SEM or otherwise, a model comparison approach is to be preferred.  Even when we don't have the greatest model, being able to choose among viable, or at least competing, options can help science progress.

#### AIC

AIC is a good way to compare models in SEM just as it is elsewhere, where a penalty is imposed on the likelihood based on the number of parameters estimated, i.e. model complexity.  The value by itself is not useful, but the 'better' model will have a lower value. A natural question arises as to how low is low enough to prefer one model over another, but this is impossible to answer because the value of AIC varies greatly with the data and model in question. However, this frees you to know which is 'better', at least in terms of AIC, while still allowing you to consider the relative merits of the models being considered.  However, if the lower AIC is associated with the simpler model, you'd be hard-pressed to justify not taking it.  You could also bootstrap an AIC difference if you really desired a statistical way to determine 'lower by enough'.  

A couple other things. Even if not nested, models must have the same variables for them to be compared, which may require you to constrain certain paths to be zero.  Also, AIC can be used to compare just-identified models to over-identified ones.

#### BIC

While BIC is very popularly used in SEM, it has known issues relative to AIC if predictive capability is concerned. Most SEM programs will give you AIC as well. As such, one can probably ignore the BIC, but they will likely agree. Note that while BIC has a Bayeisn interpretation, using it doesn't actually put you in a Bayesian context, and if you were using a Bayesian approach, WAIC or DIC would be appropriate.  If you aren't using a Bayesian approach, then AIC would likely be preferable in most circumstances.  The BIC has a different penalty than AIC, and is not a measure based on predictive performance, which is what we typically want in model selection.


#### Example

Let's compare the previous model to one without the indirect effect and in which the SES and Alienation contributions are independent (i.e. just make the previous code change to `alien67 ~ 0*ses`).  We'll use the <span class="pack">semTools</span> package for easy side by side comparison via the <span class="func">compareFit</span> function[^compareFit].


```{r wheatModComparison, echo=F, results='hide'}
# the model
wheaton.model2 <- '
  # measurement model
    ses     =~ education + sei
    alien67 =~ anomia67 + powerless67
    alien71 =~ anomia71 + powerless71
 
  # equations
    alien71 ~ alien67 + ses
    alien67 ~ 0*ses
 
  # correlated residuals
    # ses ~~ 0*alien67
    anomia67 ~~ anomia71
    powerless67 ~~ powerless71
'
 
alienationNoInd <- sem(wheaton.model2, sample.cov=wheaton.cov, sample.nobs=932)
library(semTools)
compfit = summary(compareFit(alienation, alienationNoInd))
```

```{r compareFits, eval=-2, echo=-(3:5)}
library(semTools)
compareFit(alienation, alienationNoInd)
cat('################### Nested Model Comparison #########################
                                chi df      p delta.cfi
alienation - alienationNoInd 200.28  1  <.001    0.0941')

pander::pander(compfit, split.table=Inf)
```

The first result is a likelihood ratio test. The model with no indirect path is nested within the model with a path and so this is a viable option.  It tells us essentially that adding the indirect path results in a statistically significantly better model.   AIC measures would suggest the indirect path would result in a better model as well.  In terms of internal fit indices, the model including the indirect effect appears to fit the data well (note that the .000 is actually 1.00), while the other model does not.  So now we can say that not only does our model appear to fit the data well, but is better than a plausible competitor.


### Prediction

While the fitted correlation matrix is nice to be able to obtain, it has always struck me a bit odd that one can't even predict the actual data with typical SEM software.  Part of this is due to the fact that the models regard the covariance matrix as opposed to the raw data, and that is the focus in many SEM situations. But in path analysis, measurement models, and SEM where mean structures are of focus (e.g. growth curves), it stands to reason that one would like to get predicted values and/or be able to test a model on new data. Even in more complex models, predictions can be made by fixing parameters at estimated values and supplying new data.

<span class="pack">Lavaan</span> at least does do this for you, and its <span class="func">lavPredict</span> function allows one to get predicted values for both latent and observed variables, for the current or new data.  In addition, the <span class="pack">semTools</span> package is a great resource for comparing models generally, comparing models across groups, model simulation and so forth.

### Observed covariates

While I would hope it is by now, just to be clear, SEM doesn't only have to be about structural relations among latent variables.  At any point observed covariates can be introduced to the structural model as well, and this is common practice.  As an example, the alienation model is fundamentally wrong (from a causal perspective if that's of particular interest), as it doesn't include many background or other characteristics we'd commonly collect on individuals and which might influence feelings of alienation.


### Interactions

Interactions among both observed and latent variables can be included in SEM, and have the same interpretation as they would in any regression model.  As noted previously, a common term for this in SEM terminology is <span class="emph">moderation</span>.  While many depictions in SEM suggest that one variable *moderates* another, just like with standard interactions it is arbitrary whether one says **A** interacts with/moderates **B** or vice versa, and this fact doesn't change just because we are conducting an SEM.  For latent variables, one can think of adding a latent variable whose indicator variables consists of product terms of the indicators for the latent variables we want to have an interaction. See <span class="func">indProd</span> and <span class="func">probe2WayMC</span> in the <span class="pack">semTools</span> package.


### Estimation

In everything demonstrated thus far, we have been using standard maximum likelihood to estimate the parameters.  This often may not be the best choice. The following list comes from the MPlus manual, and most of these are available in <span class="pack">lavaan</span>.

- **ML**: maximum likelihood parameter estimates with conventional standard errors and chi-square test statistic
- **MLM**: maximum likelihood parameter estimates with standard errors and a mean-adjusted chi-square test statistic that are robust to non-normality. The chi-square test statistic is also referred to as the Satorra-Bentler chi-square.
- **MLMV**: maximum likelihood parameter estimates with standard errors and a mean- and variance-adjusted chi-square test statistic that are robust to non-normality
- **MLR**: maximum likelihood parameter estimates with standard errors and a chi-square test statistic (when applicable) that are robust to non-normality and non-independence of observations when used with TYPE=COMPLEX. The MLR standard errors are computed using a sandwich estimator. The MLR chi-square test statistic is asymptotically equivalent to the Yuan-Bentler T2* test statistic.
- **MLF**: maximum likelihood parameter estimates with standard errors approximated by first-order derivatives and a conventional chi-square test statistic
- **MUML**: Muthén’s limited information parameter estimates, standard errors, and chi-square test statistic
- **WLS**: weighted least square parameter estimates with conventional standard errors and chi-square test statistic that use a full weight matrix. The WLS chi-square test statistic is also referred to as ADF when all outcome variables are continuous.
- **WLSM**: weighted least square parameter estimates using a diagonal weight matrix with standard errors and mean-adjusted chi-square test statistic that use a full weight matrix
- **WLSMV**: weighted least square parameter estimates using a diagonal weight matrix with standard errors and mean- and variance-adjusted chi-square test statistic that use a full weight matrix
- **ULS**: unweighted least squares parameter estimates
- **ULSMV**: unweighted least squares parameter estimates with standard errors and a mean- and variance-adjusted chi-square test statistic that use a full weight matrix
- **GLS**: generalized least square parameter estimates with conventional standard errors and chi-square test statistic that use a normal-theory based weight matrix
- **Bayes**: Bayesian posterior parameter estimates with credibility intervals and posterior predictive checking[^bayesEstimator]


### Missing data

A lot of data of interest in applications of SEM have missing values.  Two common approaches to dealing with this are <span class="emph">Full Information Maximum Likelihood</span> (FIML) and <span class="emph">Multiple Imputation</span> (MI), and both are generally available in SEM packages.  This is far too detailed an issue to treat adequately here, though we can take a moment to describe the approach generally.  FIML uses the available information in the data (think pairwise correlations). MI uses a process to estimate the raw data values, and to adequately account for the uncertainty in those guesses, it creates multiple versions of complete data sets, each with different estimates of the missing values.  The SEM model is run on all of them and estimates combined across all models (e.g. the mean path parameter).  The imputation models, i.e. those used to estimate the missing values, can be any sort of regression model, including using variables not in the SEM model.

In addition, Bayesian approaches can estimate the missing values as additional parameters in the model (in fact, MI is essentially steeped within a Bayesian perspective).  Also there may additional concerns when data is missing over time, i.e. longitudinal dropout.  Using the <span class="pack">lavaan</span> package is nice because it comes with FIML, and the <span class="pack">semTools</span> package adds MI.

### Other SEM approaches

SEM is very flexible and applicable to a wide variety of modeling situations.  Some of these will be covered in their own module (e.g. mixture models, growth curve modeling).


## How to fool yourself with SEM

Kline's third edition text listed over 50(!) ways in which one could fool themselves with SEM, which speaks to the difficulty in running SEM and dealing with all of its issues.  I will note a handful of some of them to keep in mind in particular.

### Sample size

If you don't have at least a thousand observations, you will probably only be able to conduct (possibly unrealistically) simple SEM models, or just the measurement models for scale development, or only structural models with observed variables (path analysis).  Even with simpler modeling situations, one should have several hundred observations.  In the simple alienation model above, we already are dealing with 17 parameters to estimate, and it doesn't include any background covariates of the individuals, which is unrealistic.  Furthermore, because it's a mediation model, adding such variables might require additional direct and indirect paths, time-varying covariates that would have effects at both years, etc., and the number of parameters could balloon quite quickly.

One will see many articles of published research with low sample sizes using SEM.  This doesn't make it correct to do so, and one should be highly suspicious of the results suggested in those papers, as they are overfit or not including relevant information.

### Poor data

If the correlations among the data are low, one isn't going to magically have strong effects by using SEM.  I have seen many clients running these models and who are surprised that they don't turn out well, when a quick glance at the correlation matrix would have suggested that there wasn't much to work with in the first place. 

### Naming a latent variable doesn't mean it exists

While everything may turn out well for one's measurement model, and the results in keeping with theory, this doesn't make it so.  This is especially the case with less reliable measures. Latent constructs require operational definitions and other considerations in order to be useful, and rule out that one isn't simply measuring something else, or that it makes sense that such a construct has real (physical ties).

As an example, many diagnoses in the Diagnostic and Statistical Manual of Mental Disorders have not even been shown to exist via a statistical approach like SEM[^dsm], while others are simply assumed to exist, and even presumably (subsequently) supported by measurement models (often with low N), only to be shown to have no ties to any underlying physiology.

### Ignoring diagnostics

Ignoring residuals, warning messages, etc. is a sure path to trying to interpret nonsensical results.  Look at your residuals, fitted values etc. If your SEM software of choice is giving you messages, find out what they mean, because it may be very important.

### Ignoring performance

As in our previous path analysis example, one can write a paper on a good fitting model with statistically significant results, and not explain the targets of interest very well on a practical level. Check things like R-square (and accuracy if binary targets) when running your models.

## Summary

SEM is a powerful modeling approach that generalizes many other techniques, but it simply cannot be used lightly. Strong theory, strong data, and a lot of data can potentially result in quite interesting models that have a lot to say about the underlying constructs of interest.  Go into it with competing ideas, and realize that your theory is wrong from the outset, even if there is evidence that it isn't way off.


## R packages used

- lavaan
- semTools
- semPlots


[^bpSEM]: Bollen & Pearl (2013) put the causal aspect this way: 'SEM is an inference engine that takes in two inputs, qualitative causal assumptions and empirical data, and derives two logical consequences of these inputs: quantitative causal conclusions and statistical measures of fit for the testable implications of the assumptions. Failure to fit the data casts doubt on the strong causal assumptions of zero coefficients or zero covariances and guides the researcher to diagnose, or repair the structural misspecifications. Fitting the data does not "prove" the causal assumptions, but it makes them tentatively more plausible. Any such positive results need to be replicated and to withstand the criticisms of researchers who suggest other models for the same data.'.

[^notv]: For example, practically every growth curve model I've ever come across.  I cringe every time I see one that doesn't include time-varying covariates, though I know why they aren't there.

[^notbayes]: And not in a Bayesian approach where it might be more viable.

[^bayesEstimator]: See the <span class="pack">blavaan</span> package.

[^dsm]: Despite the name, there is nothing inherently 'statistical' about the DSM.

[^compareFit]: This function, while very useful, incorrectly notes which fit is 'better' in terms of fit indices that cannot be used to compare models, and furthermore provides no way to turn the flag off.

[^fitkenny]: See also this [site](http://davidakenny.net/cm/fit.htm), which seems to somehow have escaped the dissolution of GeoCities webpages.