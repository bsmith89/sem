# Item Response Theory

<span class="emph">Item Response Theory</span> (IRT) is a class of latent variable models with a long history in the testing environment (e.g. scholastic aptitude), but are actually a more general latent variable approach that might be applicable to a wide variety  of settings.  In the typical scenario, we might have a set of test items which are simply binary indicators for whether the item was answered correctly.  The relationship between IRT and SEM comes in the form of a specific type of factor analysis depending on the type of IRT model being considered.

## Standard Models

We can begin our understanding of IRT with an example with a logistic regression model.

$$g(\mu) = X\beta$$
$$\pi = g(\mu)^{-1}$$
$$y \sim \mathrm{Bernoulli}(\pi)$$

The link function $g(.)$ is the logistic function, and its inverse maps our linear predictor, the logit, or log odds ($\ln(\pi)/\ln(1-\pi)$), to the probability scale.  Finally, our binary response is bernoulli distributed (i.e. binomial with size=1).  Let's see this for a single observation to remove any mystery.

```{r}
logit = -1
exp(logit)/(1+exp(logit))  # convert logit to probability
1/(1+exp(-logit))          # convert logit to probability (alternate)
plogis(logit)

prob = .75
logit = log(.75/(1-.75))   # convert probability to logit
plogis(logit)
```

Now let's speak more generally, and say that our response $y$ is the probability that a person answers correctly. In terms of a logistic regression model: 

$$prob(y=1) = f(X)$$
In other words, the probability of choosing the correct response (or simply endorsing an attitude or many other scenarios), $y=1$, is some function of the X variables, which will at a minimum be the items for an IRT model.


### 1 Parameter Model

We now turn to specific IRT models.  The one-parameter, a.k.a. *Rasch*, model (1PM) can be expressed as follows:

$$prob(y=1|\theta, \delta) = \frac{1}{1+e^{\delta_j-\theta_i}}$$

$$prob(y=1|\theta, \delta) = \frac{1}{1+e^{\delta_j-\theta_i}}$$

In this setting, the probability of endorsement (or getting an item correct) is a function of the difficulty of item $j$, $\delta_j$ above, and the latent trait (ability) of person $i$, $\theta_i$.  In terms of testing, a person with more 'ability' relative to the item difficulty will answer correctly.  In terms of the logit:

$$\mathrm{logit_{ij}} = \mathrm{log}(\frac{\pi_{ij}}{1-\pi_{ij}})$$



IRT often utilizes a different parameterization, though the results are the same.  

There is an additional parameter, $\alpha$, item discrimination, which refers to the item's ability to distinguish one person from another. In the Rasch model it is held constant, and originally at 1. If we add it to the mix we have:

$$prob(y=1|\theta, \delta) = \frac{1}{1+e^{\alpha(\delta_j-\theta_i)}}$$
As we will see later, the two parameter IRT model estimates the discrimination parameter of item. Note also, the ltm package we will use doesn't fix the discrimination parameter to be 1 in the model, so you'll actually have an estimate for it, but it's still constant across items.


```{r ltm_1pm}
library(ltm); library(tidyverse)
rasch_mod1 = rasch(Abortion, IRT.param=F)
rasch_mod2 = rasch(Abortion, IRT.param=T)
# rasch_mod = rasch(Abortion, IRT.param=T, constraint = cbind(ncol(Abortion) + 1, 1))
rasch_mod1
```

To me it makes more sense to think of this model from the perspective of a mixed model.  In this approach, we can put the data in long form such that multiple rows/observations pertain to an individual's response for the items. We then run a mixed model predicting the binary outcome with a fixed effect for item and a random effect for person. The fixed effects for item represent item difficulty, while The latent trait in the IRT for the person is the random effect for that person.

```{r mixedModel_1pm}
## See https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004668.html
library(lme4)
dlong = gather(data.frame(Subject=1:nrow(Abortion), Abortion), key=Item, value=Response, -Subject) %>% 
  arrange(Subject, Item)
head(dlong)
lme_rasch = glmer(Response~ -1 + Item + (1|Subject) , dlong, family=binomial)
summary(lme_rasch, cor=F)
```

Aside from the estimation approach, the only difference is that the IRT model assumes the latent ability of person is distributed as standard normal, and estimates the discrimination parameter as a multiplier of that ability ($\alpha*N(0,1)$).  The mixed model on the other hand assumes that the random effects are distributed as normal with mean zero and standard deviation equal to the discrimination parameter ($N(0,\alpha)$. 

Comparing the fixed effects, they are quite similar.

```{r irt_mm1}
data.frame(ltm=coef(rasch_mod1)[,'beta.i'], lme=fixef(lme_rasch))
```

Same with the latent individual scores. Since the Abortion data is essentially ordered by pattern of response, I'll mix it up a little bit.

```{r irt_mm2}
idx = sample(1:nrow(Abortion))
discr = rasch_mod1$coefficients[1,2]
iscore_rasch = factor.scores(rasch_mod1, resp.patterns=Abortion)[[1]]$z1
iscore_lme = ranef(lme_rasch)$Subject[,1]
head(data.frame(ltm=discr*iscore_rasch[idx], lmer=iscore_lme[idx]))
head(data.frame(ltm=iscore_rasch[idx], lmer=iscore_lme[idx]/lme_rasch@theta))
```

And probability of item endorsement.  In the mixed effect model, these are the unconditional estimated probabilities, i.e. those that ignore the individual-specific effect.

```{r irt_mm3}
unc_prob = plogis(fixef(lme_rasch))
data.frame(coef(rasch_mod1, prob=T), unc_prob)
```

And probability of person endorsement. In the mixed effect model, these are the estimated probabilities conditional on the individual.

```{r irt_mm3}
lme_cond_prob = fitted(lme_rasch)
rasch_cond_prob = data.frame(Subject=1:nrow(Abortion), fitted(rasch_mod1,resp.patterns=Abortion, type='c'))
rasch_cond_prob = gather(rasch_cond_prob, key=Item, value=Response, -Subject) %>% 
  arrange(Subject, Item)
head(data.frame(rasch_cond_prob[idx,], lme=cond_prob[idx]))
```

### 2 Parameter Model


```{r dataSetup, echo=FALSE, eval=FALSE}
# see also: SEM in R book https://blogs.baylor.edu/rlatentvariable/sample-page/r-syntax/#Item_response_theory_models
# Item information is ratio of squared loading to uniqueness
data(Abortion, package='ltm')
library(lavaan)
mf = mutate_all(Abortion, ordered)
mfn = mutate_all(Abortion, as.numeric)
colnames(mf) = colnames(mfn) = paste0('Item_', 1:4)
# model_code = '
#   z1 =~ Item_1 + Item_2 + Item_3 + Item_4 + Item_5 + Item_6 + Item_7 + Item_8
#   z1~~ 1*z1
# '
# test_lavaan = cfa(model_code, data=mf, std.lv=T)
# test_ltm = cfa(model_code, data=mfn, std.lv=T)
# summary(test_ltm)
```

```{r ltm2p}
# logit link used for ltm
library(ltm)
test_ltm = ltm(Abortion ~ z1, IRT.param=T)
coef(test_ltm)
detach(package:ltm); detach(package:MASS) # MC longs for the day folks realize MASS is outdated and conflicts with everything
```

```{r psych2p}

# psych is akin to probit
test_psych = psych::irt.fa(Abortion, fm='ML', plot=F) # longs for the day folks realize MASS is outdated
loadings(test_psych$fa)
test_psych$irt
test_psych$tau # equal to thresholds in lavaan cfa
```

```{r lavaan2p}
twoP.model = '
# loadings
Theta =~ l1*Item_1 + l2*Item_2 + l3*Item_3 + l4*Item_4

# thresholds
Item_1 | th1*t1
Item_2 | th2*t1
Item_3 | th3*t1
Item_4 | th4*t1

# convert loadings to discrimination  (or comment out and use loadings from parameterization="theta")
discrm_1 := l1 / sqrt(1-l1^2)
discrm_2 := l2 / sqrt(1-l2^2)
discrm_3 := l3 / sqrt(1-l3^2)
discrm_4 := l4 / sqrt(1-l4^2)


# use thresholds to get difficulty    (or comment out and use thresholds from parameterization="theta")
diff_1 := th1 / sqrt(1-l1^2)
diff_2 := th2 / sqrt(1-l2^2)
diff_3 := th3 / sqrt(1-l3^2)
diff_4 := th4 / sqrt(1-l4^2)

'
twoP.fit <- cfa(twoP.model, data=mf, std.lv=T)
# twoP.fit <- cfa(twoP.model, data=mf, std.lv=T, parameterization='theta')
summary(twoP.fit)
```

```{r compareResults}
lavpars = partable(twoP.fit)

library(tidyverse)
loadings_ = data.frame(lavaan=slice(lavpars, 1:4) %>% 
                         select(est), 
                        psych=test_psych$fa$loadings[,1])
thresholds = data.frame(lavaan=filter(lavpars, rhs=='t1') %>% 
                          select(est), 
                        psych=test_psych$tau)

difficulties = data.frame(ltm=coef(test_ltm)[,1],
                          lavaan= lavpars %>% 
                            slice(grep(.$label, pattern='diff')) %>% 
                            select(est),
                          psych=test_psych$irt$difficulty[[1]])

discrimination = data.frame(ltm=coef(test_ltm)[,2]/sqrt(pi^2/3),
                            lavaan= lavpars %>% 
                              slice(grep(.$label, pattern='discr')) %>% 
                              select(est),
                            psych=test_psych$irt$discrimination)
scores_ = data.frame(ltm = ltm::factor.scores.ltm(test_ltm, resp.patterns=Abortion, method='EAP')$score.dat$z1,
                     lavaan = lavPredict(twoP.fit),            # lavaan takes a long time
                     psych = psych::factor.scores(x=Abortion, f=test_psych$fa)$scores[,1])

loadings_
thresholds
difficulties
discrimination
head(scores_, 20)
cor(scores_)
cor(scores_, method='spearman') # how  similarly would they rank people
qplot(data=gather(data.frame(scale(scores_)), key='method', value='score'), x=score, color=method, geom='density')
```




```{r misc, echo=FALSE, eval=FALSE}
ltm0
ltm(Abortion~z1, IRT.param=F, control=list(iter.em=100, iter.qN=300))

twoP.model<-'
# loadings
Theta =~ l1*Item_1 + l2*Item_2 + l3*Item_3 + l4*Item_4 + l5*Item_5 + l6*Item_6 + l7*Item_7 + l8*Item_8

# thresholds
Item_1 | th1*t1
Item_2 | th2*t1
Item_3 | th3*t1
Item_4 | th4*t1
Item_5 | th5*t1
Item_6 | th6*t1
Item_7 | th7*t1
Item_8 | th8*t1

# convert loadings to slopes (normal)
alpha1.N := (l1)/sqrt(1-l1^2)
alpha2.N := (l2)/sqrt(1-l2^2)
alpha3.N := (l3)/sqrt(1-l3^2)
alpha4.N := (l4)/sqrt(1-l4^2)
alpha5.N := (l5)/sqrt(1-l5^2)
alpha6.N := (l6)/sqrt(1-l6^2)
alpha7.N := (l7)/sqrt(1-l7^2)
alpha8.N := (l8)/sqrt(1-l8^2)

# convert thresholds to intercepts (normal)
beta1.N := (-th1)/sqrt(1-l1^2)
beta2.N := (-th2)/sqrt(1-l2^2)
beta3.N := (-th3)/sqrt(1-l3^2)
beta4.N := (-th4)/sqrt(1-l4^2)
beta5.N := (-th5)/sqrt(1-l5^2)
beta6.N := (-th6)/sqrt(1-l6^2)
beta7.N := (-th7)/sqrt(1-l7^2)
beta8.N := (-th8)/sqrt(1-l8^2)

# convert intercepts to locations (normal)
loc1 := -beta1.N/alpha1.N
loc2 := -beta2.N/alpha2.N
loc3 := -beta3.N/alpha3.N
loc4 := -beta4.N/alpha4.N
loc5 := -beta5.N/alpha5.N
loc6 := -beta6.N/alpha6.N
loc7 := -beta7.N/alpha7.N
loc8 := -beta8.N/alpha8.N

# convert loadings to slopes (logistic)
alpha1.L := (l1)/sqrt(1-l1^2)*1.7
alpha2.L := (l2)/sqrt(1-l2^2)*1.7
alpha3.L := (l3)/sqrt(1-l3^2)*1.7
alpha4.L := (l4)/sqrt(1-l4^2)*1.7
alpha5.L := (l5)/sqrt(1-l5^2)*1.7
alpha6.L := (l6)/sqrt(1-l6^2)*1.7
alpha7.L := (l7)/sqrt(1-l7^2)*1.7
alpha8.L := (l8)/sqrt(1-l8^2)*1.7

# convert thresholds to locations (logistic)
loc1.L := th1/l1
loc2.L := th2/l2
loc3.L := th3/l3
loc4.L := th4/l4
loc5.L := th5/l5
loc6.L := th6/l6
loc7.L := th7/l7
loc8.L := th8/l8

# convert locations to intercepts (logistic)
beta1.L := (-alpha1.L)*loc1.L
beta2.L := (-alpha2.L)*loc2.L
beta3.L := (-alpha3.L)*loc3.L
beta4.L := (-alpha4.L)*loc4.L
beta5.L := (-alpha5.L)*loc5.L
beta6.L := (-alpha6.L)*loc6.L
beta7.L := (-alpha7.L)*loc7.L
beta8.L := (-alpha8.L)*loc8.L
'

twoP.fit <- cfa(twoP.model, data=mf, std.lv=T, link='probit')
summary(twoP.fit)

```




#### Graded Response Model
### 3 Parameter Model
### 4 Parameter Model

## Terminology

## Summary

## R Packages Used