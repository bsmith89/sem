# Item Response Theory

<span class="emph">Item Response Theory</span> (IRT) is a class of latent variable models with a long history in the testing environment (e.g. scholastic aptitude), but are actually a more general latent variable approach that might be applicable to a wide variety  of settings.  In the typical scenario, we might have a set of test items which are simply binary indicators for whether the item was answered correctly.  The relationship between IRT and SEM comes in the form of a specific type of factor analysis depending on the type of IRT model being considered.

## Standard Models

We can begin our understanding of IRT with an example with a logistic regression model.

$$g(\mu) = X\beta$$
$$\pi = g(\mu)^{-1}$$
$$y \sim \mathrm{Bernoulli}(\pi)$$

The link function $g(.)$ is the logistic function, and its inverse maps our linear predictor, the logit, or log odds ($\ln(\pi)/\ln(1-\pi)$), to the probability scale.  Finally, our binary response is bernoulli distributed (i.e. binomial with size=1).  Let's see this for a single observation to remove any mystery.

```{r}
logit = -1
exp(logit)/(1+exp(logit))  # convert logit to probability
1/(1+exp(-logit))          # convert logit to probability (alternate)
plogis(logit)

prob = .75
logit = log(.75/(1-.75))   # convert probability to logit
plogis(logit)
```

Now let's speak more generally, and say that our response $y$ is the probability that a person answers correctly. In terms of a logistic regression model: 

$$prob(y=1) = f(X)$$

In other words, the probability of choosing the correct response (or simply endorsing an attitude or many other scenarios), $y=1$, is some function of the X variables, which will at a minimum be the items for an IRT model.


### 1 Parameter Model

We now turn to specific IRT models.  The one-parameter, a.k.a. *Rasch*, model (1PM) can be expressed as follows:

$$prob(y=1|\theta, \delta) = \frac{1}{1+e^{\delta_j-\theta_i}}$$

$$prob(y=1|\theta, \delta) = \frac{1}{1+e^{\delta_j-\theta_i}}$$

In this setting, the probability of endorsement (or getting an item correct) is a function of the difficulty of item $j$, $\delta_j$ above, and the latent trait (ability) of person $i$, $\theta_i$.  In terms of testing, a person with more 'ability' relative to the item difficulty will answer correctly.  In terms of the logit:

$$\mathrm{logit_{ij}} = \mathrm{log}(\frac{\pi_{ij}}{1-\pi_{ij}})$$



IRT often utilizes a different parameterization, though the results are the same.  

There is an additional parameter, $\alpha$, item discrimination, which refers to the item's ability to distinguish one person from another. In the Rasch model it is held constant, and in its original formulation it was fixed at 1. If we add it to the mix we have:

$$prob(y=1|\theta, \delta) = \frac{1}{1+e^{\alpha(\delta_j-\theta_i)}}$$

As we will see later, the two parameter IRT model estimates the discrimination parameter of item. Note also, the ltm package we will use doesn't fix the discrimination parameter to be 1 in the model, so you'll actually have an estimate for it, but it's still constant across items.


```{r ltm_1pm}
library(ltm); library(tidyverse)
data(Abortion, package='ltm')

mf = mutate_all(Abortion, ordered)                # for later, convert to ordered factors
mfn = mutate_all(Abortion, as.numeric)            # for comparison to standard sem if desired
colnames(mf) = colnames(mfn) = paste0('Item_', 1:4)

rasch_mod_par1 = rasch(Abortion, IRT.param=F)
rasch_mod_par2 = rasch(Abortion, IRT.param=T)
# rasch_mod_original = rasch(Abortion, IRT.param=F, constraint = cbind(ncol(Abortion) + 1, 1))
rasch_mod_par1
rasch_mod_par2
```


#### IRT as a Mixed Model

To me it makes more sense to think of this model from the perspective of a mixed model.  In this approach, we can put the data in long form such that multiple rows/observations pertain to an individual's response for the items. We then run a mixed model predicting the binary outcome with a fixed effect for item and a random effect for person. The fixed effects for item represent item difficulty, while The latent trait in the IRT for the person is the random effect for that person.

```{r mixedModel_1pm}
## See https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004668.html
library(lme4)
Abortion_long = gather(data.frame(Subject=1:nrow(Abortion), Abortion), key=Item, value=Response, -Subject) %>% 
  arrange(Subject, Item)
head(Abortion_long, 8)
lme_rasch = glmer(Response ~ -1 + Item + (1|Subject) , Abortion_long, family=binomial(link='logit'))
summary(lme_rasch, cor=F)
```

Aside from the estimation approach, the only difference is that the IRT model assumes the latent ability of person is distributed as standard normal, and estimates the discrimination parameter as a multiplier of that ability ($\alpha*N(0,1)$).  The mixed model on the other hand assumes that the random effects are distributed as normal with mean zero and standard deviation equal to the discrimination parameter ($N(0,\alpha)$. 

Comparing the fixed effects, they are quite similar.

```{r irt_mm1}
data.frame(ltm=coef(rasch_mod_par1)[,'beta.i'], lme=fixef(lme_rasch))
```

Same goes for the latent individual scores. Since the Abortion data is essentially ordered by pattern of response, I'll mix it up a little bit. As the results are on different scales, we can alternate rescaling one or the other to put them on equal footing. The correlation of the scores is essentially 1.0.

```{r irt_mm2}
idx = sample(1:nrow(Abortion))
discr = rasch_mod_par1$coefficients[1,2]
iscore_rasch = factor.scores(rasch_mod_par1, resp.patterns=Abortion)[[1]]$z1
iscore_lme = ranef(lme_rasch)$Subject[,1]
head(data.frame(ltm=discr*iscore_rasch[idx], lmer=iscore_lme[idx]))
head(data.frame(ltm=iscore_rasch[idx], lmer=iscore_lme[idx]/lme_rasch@theta))
```

We see the same thing with probability of item endorsement.  In the mixed effect model, these are the unconditional estimated probabilities, i.e. those that ignore the individual-specific effect. In the IRT model, these are the expected probabilities at the average latent trait score (i.e. 0).

```{r irt_mm3}
unc_prob = plogis(fixef(lme_rasch))
data.frame(ltm=coef(rasch_mod_par1, prob=T)[,'P(x=1|z=0)'], lme=unc_prob)
```

And finally, we can look at probability of person endorsement. In the mixed effect model, these are the estimated probabilities conditional on the individual. In the IRT model, they include the latent score for the individual.

```{r irt_mm3}
lme_cond_prob = fitted(lme_rasch)
rasch_cond_prob = data.frame(Subject=1:nrow(Abortion), fitted(rasch_mod_par1,resp.patterns=Abortion, type='c'))
rasch_cond_prob = gather(rasch_cond_prob, key=Item, value=Response, -Subject) %>% 
  arrange(Subject, Item)
head(data.frame(ltm=rasch_cond_prob[idx,'Response'], lme=lme_cond_prob[idx]))
```

#### IRT as SEM

Now let's look at the model from a structural equation modeling perspective.  We saw in the [growth curve modeling section][Latent Growth Curves] how a growth curve model can duplicate the results from a mixed model. It is unusual in the SEM framework in that most of the parameters are fixed.  In order to do a one-parameter model in SEM, we'll take the same approach of fixing several parameters.  We'll do so for each IRT parameterization.  We'll start with the first and compare the results to the mixed model as well.

```{r lavaan_1pm_1}
library(lavaan)
oneP.model = '
# loadings
Theta =~ l1*Item_1 + l1*Item_2 + l1*Item_3 + l1*Item_4

# thresholds
Item_1 | th1*t1
Item_2 | th2*t1
Item_3 | th3*t1
Item_4 | th4*t1

# convert loading to discrimination
discrm := l1 / sqrt(1-l1^2)

# use thresholds to get difficulty   
diff_1 := -th1 / sqrt(1-l1^2)
diff_2 := -th2 / sqrt(1-l1^2)
diff_3 := -th3 / sqrt(1-l1^2)
diff_4 := -th4 / sqrt(1-l1^2)
'
oneP.fit <- cfa(oneP.model, data=mf, std.lv=T, std.ov=T, parameterization='delta')
summary(oneP.fit)


lme_rasch_prob = glmer(Response~ -1 + Item + (1|Subject), Abortion_long, family=binomial(link='probit'))
summary(lme_rasch_prob, cor=F)
data.frame(ltm_diff = coef(rasch_mod_par1)[1:4]/1.7,
           lme_diff = fixef(lme_rasch_prob), 
           lav_diff = parameterEstimates(oneP.fit)[24:27, 'est'], 
           ltm_disc = coef(rasch_mod_par1)[5]/1.7,
           lme_disc = lme_rasch_prob@theta,
           lav_disc = parameterEstimates(oneP.fit)[23, 'est']) %>% round(2)
```

```{r lavaan_1pm_2}
oneP.model = '
# loadings
Theta =~ l1*Item_1 + l1*Item_2 + l1*Item_3 + l1*Item_4

# thresholds
Item_1 | th1*t1
Item_2 | th2*t1
Item_3 | th3*t1
Item_4 | th4*t1
# 
# # convert loading to discrimination
discrm := l1 / sqrt(1-l1^2) * 1.7 # see http://andrewgelman.com/2006/06/06/take_logit_coef/ and https://www.jstor.org/stable/1165298?seq=1#page_scan_tab_contents

'
oneP.fit <- cfa(oneP.model, data=mf, std.lv=T, std.ov=F)

summary(oneP.fit)
data.frame(ltm=coef(rasch_mod_par2), 
           lav=coef(oneP.fit)[-(1:4)], 
           discrm=parameterEstimates(oneP.fit)[23,'est']) %>% round(2)
```






### 2 Parameter Model


```{r dataSetup, echo=FALSE, eval=FALSE}
# see also: SEM in R book https://blogs.baylor.edu/rlatentvariable/sample-page/r-syntax/#Item_response_theory_models
# Item information is ratio of squared loading to uniqueness

# model_code = '
#   z1 =~ Item_1 + Item_2 + Item_3 + Item_4 + Item_5 + Item_6 + Item_7 + Item_8
#   z1~~ 1*z1
# '
# test_lavaan = cfa(model_code, data=mf, std.lv=T)
# test_ltm = cfa(model_code, data=mfn, std.lv=T)
# summary(test_ltm)
```

```{r ltm2p}
# logit link used for ltm
library(ltm)
test_ltm = ltm(Abortion ~ z1, IRT.param=T)
coef(test_ltm)
detach(package:ltm); detach(package:MASS) # MC longs for the day folks realize MASS is outdated and conflicts with everything
```

```{r psych2p}

# psych is akin to probit
test_psych = psych::irt.fa(Abortion, fm='ML', plot=F) # longs for the day folks realize MASS is outdated
loadings(test_psych$fa)
test_psych$irt
test_psych$tau # equal to thresholds in lavaan cfa
```

```{r lavaan2p}
twoP.model = '
# loadings
Theta =~ l1*Item_1 + l2*Item_2 + l3*Item_3 + l4*Item_4

# thresholds
Item_1 | th1*t1
Item_2 | th2*t1
Item_3 | th3*t1
Item_4 | th4*t1

# convert loadings to discrimination  (or comment out and use loadings from parameterization="theta")
discrm_1 := l1 / sqrt(1-l1^2)
discrm_2 := l2 / sqrt(1-l2^2)
discrm_3 := l3 / sqrt(1-l3^2)
discrm_4 := l4 / sqrt(1-l4^2)


# use thresholds to get difficulty    (or comment out and use thresholds from parameterization="theta")
diff_1 := th1 / sqrt(1-l1^2)
diff_2 := th2 / sqrt(1-l2^2)
diff_3 := th3 / sqrt(1-l3^2)
diff_4 := th4 / sqrt(1-l4^2)

'
twoP.fit <- cfa(twoP.model, data=mf, std.lv=T)
# twoP.fit <- cfa(twoP.model, data=mf, std.lv=T, parameterization='theta')
summary(twoP.fit)
```

```{r compareResults}
lavpars = partable(twoP.fit)

library(tidyverse)
loadings_ = data.frame(lavaan=slice(lavpars, 1:4) %>% 
                         select(est), 
                        psych=test_psych$fa$loadings[,1])
thresholds = data.frame(lavaan=filter(lavpars, rhs=='t1') %>% 
                          select(est), 
                        psych=test_psych$tau)

difficulties = data.frame(ltm=coef(test_ltm)[,1],
                          lavaan= lavpars %>% 
                            slice(grep(.$label, pattern='diff')) %>% 
                            select(est),
                          psych=test_psych$irt$difficulty[[1]])

discrimination = data.frame(ltm=coef(test_ltm)[,2]/sqrt(pi^2/3),
                            lavaan= lavpars %>% 
                              slice(grep(.$label, pattern='discr')) %>% 
                              select(est),
                            psych=test_psych$irt$discrimination)
scores_ = data.frame(ltm = ltm::factor.scores.ltm(test_ltm, resp.patterns=Abortion, method='EAP')$score.dat$z1,
                     lavaan = lavPredict(twoP.fit),            # lavaan takes a long time
                     psych = psych::factor.scores(x=Abortion, f=test_psych$fa)$scores[,1])

loadings_
thresholds
difficulties
discrimination
head(scores_, 20)
cor(scores_)
cor(scores_, method='spearman') # how  similarly would they rank people
qplot(data=gather(data.frame(scale(scores_)), key='method', value='score'), x=score, color=method, geom='density')
```




```{r misc, echo=FALSE, eval=FALSE}
ltm0
ltm(Abortion~z1, IRT.param=F, control=list(iter.em=100, iter.qN=300))

twoP.model<-'
# loadings
Theta =~ l1*Item_1 + l2*Item_2 + l3*Item_3 + l4*Item_4 + l5*Item_5 + l6*Item_6 + l7*Item_7 + l8*Item_8

# thresholds
Item_1 | th1*t1
Item_2 | th2*t1
Item_3 | th3*t1
Item_4 | th4*t1
Item_5 | th5*t1
Item_6 | th6*t1
Item_7 | th7*t1
Item_8 | th8*t1

# convert loadings to slopes (normal)
alpha1.N := (l1)/sqrt(1-l1^2)
alpha2.N := (l2)/sqrt(1-l2^2)
alpha3.N := (l3)/sqrt(1-l3^2)
alpha4.N := (l4)/sqrt(1-l4^2)
alpha5.N := (l5)/sqrt(1-l5^2)
alpha6.N := (l6)/sqrt(1-l6^2)
alpha7.N := (l7)/sqrt(1-l7^2)
alpha8.N := (l8)/sqrt(1-l8^2)

# convert thresholds to intercepts (normal)
beta1.N := (-th1)/sqrt(1-l1^2)
beta2.N := (-th2)/sqrt(1-l2^2)
beta3.N := (-th3)/sqrt(1-l3^2)
beta4.N := (-th4)/sqrt(1-l4^2)
beta5.N := (-th5)/sqrt(1-l5^2)
beta6.N := (-th6)/sqrt(1-l6^2)
beta7.N := (-th7)/sqrt(1-l7^2)
beta8.N := (-th8)/sqrt(1-l8^2)

# convert intercepts to locations (normal)
loc1 := -beta1.N/alpha1.N
loc2 := -beta2.N/alpha2.N
loc3 := -beta3.N/alpha3.N
loc4 := -beta4.N/alpha4.N
loc5 := -beta5.N/alpha5.N
loc6 := -beta6.N/alpha6.N
loc7 := -beta7.N/alpha7.N
loc8 := -beta8.N/alpha8.N

# convert loadings to slopes (logistic)
alpha1.L := (l1)/sqrt(1-l1^2)*1.7
alpha2.L := (l2)/sqrt(1-l2^2)*1.7
alpha3.L := (l3)/sqrt(1-l3^2)*1.7
alpha4.L := (l4)/sqrt(1-l4^2)*1.7
alpha5.L := (l5)/sqrt(1-l5^2)*1.7
alpha6.L := (l6)/sqrt(1-l6^2)*1.7
alpha7.L := (l7)/sqrt(1-l7^2)*1.7
alpha8.L := (l8)/sqrt(1-l8^2)*1.7

# convert thresholds to locations (logistic)
loc1.L := th1/l1
loc2.L := th2/l2
loc3.L := th3/l3
loc4.L := th4/l4
loc5.L := th5/l5
loc6.L := th6/l6
loc7.L := th7/l7
loc8.L := th8/l8

# convert locations to intercepts (logistic)
beta1.L := (-alpha1.L)*loc1.L
beta2.L := (-alpha2.L)*loc2.L
beta3.L := (-alpha3.L)*loc3.L
beta4.L := (-alpha4.L)*loc4.L
beta5.L := (-alpha5.L)*loc5.L
beta6.L := (-alpha6.L)*loc6.L
beta7.L := (-alpha7.L)*loc7.L
beta8.L := (-alpha8.L)*loc8.L
'

twoP.fit <- cfa(twoP.model, data=mf, std.lv=T, link='probit')
summary(twoP.fit)

```




#### Graded Response Model
### 3 Parameter Model
### 4 Parameter Model

## Terminology

## Summary

## R Packages Used